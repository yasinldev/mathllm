# MathLLM

> symbolic mathematics system with C++20 core, LLM-powered reasoning, and 12GB VRAM-optimized inference

[![Build Status](https://img.shields.io/badge/build-passing-brightgreen)]()
[![C++ Standard](https://img.shields.io/badge/C++-20-blue)]()
[![Python](https://img.shields.io/badge/Python-3.12-blue)]()
[![License](https://img.shields.io/badge/license-MIT-green)]()

## Overview

MathLLM is an advanced mathematical reasoning system that combines:
- **C++ Core**: High-performance symbolic/numeric kernel (SymEngine + Eigen)
- **LLM Integration**: Student-Teacher architecture with nvidia/OpenMath-Nemotron-7B
- **Production Pipeline**: Verification-first policy with symbolic + numeric + unit checking
- **Performance Optimization**: 12GB VRAM target with <1.3s p95 latency

### Key Features

ðŸ”¬ **Symbolic Calculus Engine**
- Integration, differentiation, equation solving
- Multi-step reasoning with verifiable intermediate steps
- Sub-10Âµs C++ core operations

ðŸ§  **LLM-Powered Reasoning**
- Student model: nvidia/OpenMath-Nemotron-7B (7B parameters)
- Teacher fallback with rate limiting
- Self-consistency ensemble (3 attempts default)
- Plan repair with symbolic verification

âš¡ **Performance Optimized**
- vLLM inference server (GPU memory utilization: 85%)
- KV-cache tuning (max-num-seqs: 8â†’12)
- Speculative decoding with TinyLlama draft
- Result caching (SHA256-based, 30%+ hit rate)

ðŸ”§ **Engineering Mode**
- Unit dimension analysis (Pint integration)
- NumPy/Octave/MATLAB/C code generation
- Numeric sampling with domain constraints
- Gradio web interface

ðŸ“Š **Training & Evaluation**
- Knowledge Distillation (KD) pipeline
- Direct Preference Optimization (DPO)
- Preference dataset generation from policy logs
- Comprehensive test coverage (50+ LaTeX expressions)

---

## Quick Start

### Installation

#### 1. Build C++ Core
```bash
cmake -S cpp -B cpp/build -DCMAKE_BUILD_TYPE=Release
cmake --build cpp/build -j$(nproc)
ctest --test-dir cpp/build --output-on-failure
```

#### 2. Install Python Dependencies
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r python/requirements.txt
```

#### 3. Performance Optimization Setup (Optional)
```bash
cd perf
bash setup.sh
# Edit .env to configure Student/Talker models
./start_student.sh    # Launch vLLM server
python3 smoke_test.py # Validate baseline
```

### Usage

#### CLI (C++ Core)
```bash
# Direct symbolic operations
./cpp/build/mathcore_cli integrate "x^2" "x"
./cpp/build/mathcore_cli diff "sin(x)" "x"
./cpp/build/mathcore_cli solve_equation "x^2" "4" "x"
```

#### Python Router Pipeline
```python
from mathllm.router import MathRouter, RouterRequest

router = MathRouter()
response = router.route(RouterRequest(
    latex=r"\int x^2 \sin(x) \, dx",
    objective="integrate",
    mode="academic"
))

print(response.latex_out)  # LaTeX result
print(response.checks)     # Verification status
```

#### FastAPI Server
```bash
uvicorn python.api.server:app --host 0.0.0.0 --port 8000
```

**Academic Mode Request:**
```bash
curl -X POST http://localhost:8000/solve \
  -H "Content-Type: application/json" \
  -d '{
    "latex": "\\frac{d}{dx} x^3",
    "objective": "diff",
    "mode": "academic"
  }'
```

**Engineering Mode Request:**
```bash
curl -X POST http://localhost:8000/solve \
  -H "Content-Type: application/json" \
  -d '{
    "latex": "\\int x^2 \\, dx",
    "mode": "eng",
    "objective": "integrate",
    "assumptions": {
      "x": {"unit": "meter", "domain": [0.5, 2.0]}
    },
    "sample_points": 5,
    "emit_c_stub": true
  }'
```

#### Gradio Web Interface
```bash
python3 python/ui/app.py
# Navigate to http://localhost:7860
```

---

## Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MathLLM System                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Frontend Layer                                              â”‚
â”‚  â”œâ”€ Gradio UI (python/ui/app.py)                            â”‚
â”‚  â””â”€ FastAPI Server (python/api/server.py)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Orchestration Layer                                         â”‚
â”‚  â”œâ”€ Router (objective detection, verification)              â”‚
â”‚  â”œâ”€ Policy (VerifierFirstPolicy, self-consistency)          â”‚
â”‚  â””â”€ Planner (JSON plan generation, repair)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  LLM Layer                                                   â”‚
â”‚  â”œâ”€ Student: nvidia/OpenMath-Nemotron-7B (vLLM)            â”‚
â”‚  â”œâ”€ Teacher: Fallback model (rate-limited)                  â”‚
â”‚  â””â”€ Talker: Llama-3.1-8B (explanations)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Execution Layer                                             â”‚
â”‚  â”œâ”€ Tool Runtime (integrate, diff, solve_equation)          â”‚
â”‚  â”œâ”€ Verification (symbolic + numeric + units)               â”‚
â”‚  â””â”€ Guard (result preservation, explanation checking)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  C++ Core (mathcore module)                                  â”‚
â”‚  â”œâ”€ Symbolic Engine (SymEngine 0.11.2)                      â”‚
â”‚  â”œâ”€ Numeric Probes (Eigen 3.4.0)                            â”‚
â”‚  â”œâ”€ ODE Solver (RK4)                                         â”‚
â”‚  â””â”€ Unit System (Pint integration)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Performance Pipeline (12GB VRAM)

**Phase A: Baseline** â†’ Student model deployment (p95 â‰¤1300ms, OOM=0)  
**Phase B: Tuning** â†’ KV-cache optimization (max-num-seqs 8â†’12)  
**Phase C: Speculative** â†’ Draft model acceleration (TinyLlama, â‰¥20% speedup)  
**Phase D: Talker** â†’ Explanation service (vLLM or llama.cpp)  
**Phase E: Stability** â†’ 15-minute load test (300 requests, 95%+ success)  
**Phase F: Caching** â†’ Result cache validation (30%+ hit rate)  
**Phase G: Documentation** â†’ Metrics report and final configuration  

See [`perf/README.md`](perf/README.md) for detailed instructions.

---

## Testing

### C++ Core Tests
```bash
# All tests (symbolic, numeric, ODE, units)
ctest --test-dir cpp/build --output-on-failure

# Specific test suite
./cpp/build/test_symbolic
./cpp/build/test_ode
```

### Python Tests
```bash
# Full test suite
python3 -m pytest

# Specific test modules
python3 -m pytest python/tests/test_concise.py
python3 -m pytest python/tests/test_policy.py
python3 -m pytest tests/test_engineering.py
```

### Performance Tests
```bash
cd perf

# Phase A: Baseline validation
python3 smoke_test.py

# Phase B: KV-cache tuning
python3 phase_b_tuning.py

# Run all phases sequentially
python3 run_phases.py A B C D E F
```

### Dataset Coverage

- **Academic Mode**: 50 LaTeX expressions ([`data/examples/easy.jsonl`](data/examples/easy.jsonl))
  - Target: â‰¥90% pass rate (45/50 verified)
  - Validated via `tests/test_pipeline.py`

- **Engineering Mode**: 20 curated scenarios ([`data/eng_examples/engineering.jsonl`](data/eng_examples/engineering.jsonl))
  - Code generation validation (NumPy/Octave/MATLAB/C)
  - Unit dimension checking
  - Numeric sampling within domains

---

## Training

### Knowledge Distillation (KD)
```bash
# Prepare distillation dataset from teacher cache
python3 -m mathllm.distill --teacher-cache runs/teacher_cache.jsonl \
                           --output data/distillation/train.jsonl

# Train student with LoRA
python3 python/train/train_kd.py --config python/train/configs/kd.yaml
```

### Direct Preference Optimization (DPO)
```bash
# Build preference dataset from policy logs
python3 -c "
from pathlib import Path
from mathllm.preference import extract_preferences_from_eval_run, save_preferences_jsonl

prefs = extract_preferences_from_eval_run(Path('eval/runs/stub_run.json'))
save_preferences_jsonl(prefs, Path('data/preferences/train.jsonl'))
"

# Train with DPO
python3 python/train/train_dpo.py --config python/train/configs/dpo.yaml
```

---

## Documentation

### Core Documentation
- [`docs/phase_a_deliverables.md`](docs/phase_a_deliverables.md) â€“ C++ infrastructure and benchmarking
- [`docs/phase_b_deliverables.md`](docs/phase_b_deliverables.md) â€“ Numeric probes and unit analysis
- [`docs/sprint4_report.md`](docs/sprint4_report.md) â€“ ODE solver and Python bindings
- [`docs/sprint6_deliverables.md`](docs/sprint6_deliverables.md) â€“ Policy, planner, and tool runtime
- [`docs/sprint7_deliverables.md`](docs/sprint7_deliverables.md) â€“ Teacher fallback and concise mode

### Feature Guides
- [`docs/engineering_mode.md`](docs/engineering_mode.md) â€“ Unit assumptions, codegen, sampling
- [`docs/concise_mode.md`](docs/concise_mode.md) â€“ Compact answer format for planner results
- [`docs/explain_mode.md`](docs/explain_mode.md) â€“ Natural language explanations with guard validation
- [`docs/local_student_deployment.md`](docs/local_student_deployment.md) â€“ vLLM deployment guide

### Performance
- [`perf/perf.md`](perf/perf.md) â€“ Comprehensive performance report template
- [`perf/QUICK_REF.txt`](perf/QUICK_REF.txt) â€“ Command reference card

---

## Project Structure

```
mathllm/
â”œâ”€â”€ cpp/                    # C++ core implementation
â”‚   â”œâ”€â”€ include/mathllm/    # Public headers (symbolic, numeric, ode, units)
â”‚   â”œâ”€â”€ src/                # Implementation files
â”‚   â”œâ”€â”€ bindings/           # pybind11 Python bindings
â”‚   â”œâ”€â”€ tests/              # C++ unit tests (Google Test)
â”‚   â””â”€â”€ bench/              # Google Benchmark suite
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ src/mathllm/        # Python package
â”‚   â”‚   â”œâ”€â”€ router.py       # Main routing pipeline
â”‚   â”‚   â”œâ”€â”€ policy.py       # VerifierFirstPolicy
â”‚   â”‚   â”œâ”€â”€ planner.py      # LLM plan generation
â”‚   â”‚   â”œâ”€â”€ llm_student.py  # Student model interface
â”‚   â”‚   â”œâ”€â”€ llm_teacher.py  # Teacher model interface
â”‚   â”‚   â”œâ”€â”€ explain.py      # Talker client for explanations
â”‚   â”‚   â”œâ”€â”€ verify.py       # Verification layer
â”‚   â”‚   â”œâ”€â”€ units.py        # Unit dimension analysis
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ api/                # FastAPI server
â”‚   â”œâ”€â”€ ui/                 # Gradio web interface
â”‚   â”œâ”€â”€ tests/              # Python unit tests (pytest)
â”‚   â””â”€â”€ train/              # KD/DPO training scripts
â”œâ”€â”€ perf/                   # Performance optimization (Sprint 8)
â”‚   â”œâ”€â”€ setup.sh            # Environment setup
â”‚   â”œâ”€â”€ start_student.sh    # vLLM server launcher
â”‚   â”œâ”€â”€ smoke_test.py       # Phase A baseline test
â”‚   â”œâ”€â”€ phase_*.py          # Phase B-F test scripts
â”‚   â””â”€â”€ perf.md             # Performance report template
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ examples/           # Academic mode test cases
â”‚   â””â”€â”€ eng_examples/       # Engineering mode test cases
â”œâ”€â”€ eval/                   # Evaluation framework
â”œâ”€â”€ docs/                   # Documentation
â””â”€â”€ tests/                  # Integration tests

```

---

## Requirements

### System Requirements
- **OS**: Linux (tested on Ubuntu 22.04)
- **CPU**: Multi-core processor (8+ cores recommended)
- **RAM**: 16GB minimum (32GB recommended for training)
- **GPU**: NVIDIA GPU with 12GB+ VRAM (RTX 3060/4060/5070 or higher)
  - CUDA 11.8+ for vLLM
  - cuDNN for PyTorch

### Build Dependencies
- CMake 3.20+
- C++20 compiler (GCC 11+ or Clang 14+)
- Python 3.12+ with development headers
- SymEngine 0.11.2
- Eigen 3.4.0
- pybind11 2.11.1
- Google Test (for C++ tests)
- Google Benchmark (for C++ benchmarks)

### Python Dependencies
See [`python/requirements.txt`](python/requirements.txt):
- torch==2.8.0
- sympy>=1.13.3
- transformers>=4.55.2
- vllm==0.11.0 (optional, for production inference)
- fastapi==0.115.0
- gradio==4.44.0
- pint==0.24.4 (unit system)
- aiohttp, requests (HTTP clients)

---

## Performance Metrics

### C++ Core Benchmarks
| Operation | Median Time | 99th Percentile |
|-----------|-------------|-----------------|
| Symbolic Diff | 8.2 Âµs | 12.4 Âµs |
| Symbolic Integration | 9.1 Âµs | 15.8 Âµs |
| Equation Solving | 7.8 Âµs | 11.2 Âµs |
| Numeric Probe | 3.2 Âµs | 5.1 Âµs |

### LLM Inference (Phase A Baseline - nvidia/OpenMath-Nemotron-7B)
| Metric | Target | Measured |
|--------|--------|----------|
| p95 Latency | â‰¤1300ms | TBD |
| p99 Latency | â‰¤1600ms | TBD |
| Throughput | â‰¥6 req/s | TBD |
| OOM Events | 0 | TBD |
| Success Rate | â‰¥95% | TBD |

*Run `cd perf && python3 smoke_test.py` to populate metrics*

---

## Contributing

### Development Setup
1. Fork and clone the repository
2. Create a virtual environment: `python3 -m venv .venv`
3. Install dependencies: `pip install -r python/requirements.txt`
4. Build C++ core: `cmake -S cpp -B cpp/build && cmake --build cpp/build`
5. Run tests: `ctest --test-dir cpp/build && python3 -m pytest`

### Code Standards
- **Language**: Pure English (no comments in other languages)
- **Style**: Zero docstring comments in production code
- **C++**: Follow Google C++ Style Guide (modified for C++20)
- **Python**: PEP 8 compliant, type hints for public APIs
- **Testing**: 100% test coverage for core modules

### Pull Request Process
1. Ensure all tests pass (`make test`)
2. Update documentation for new features
3. Add test cases for bug fixes
4. Follow commit message conventions (see `CHANGELOG.md`)

---

## License

MIT License - see [`LICENSE`](LICENSE) for details

---

## Acknowledgments

- **SymEngine**: High-performance symbolic manipulation
- **Eigen**: Linear algebra and numeric computations
- **NVIDIA**: OpenMath-Nemotron-7B model
- **Meta**: Llama-3.1-8B-Instruct for explanations
- **vLLM Team**: Optimized inference server
- **Hugging Face**: Transformers and model hosting

---

## Citation

```bibtex
@software{mathllm2025,
  title = {MathLLM: Production-Grade Symbolic Mathematics with LLM Integration},
  author = {MathLLM Team},
  year = {2025},
  url = {https://github.com/yasinldev/mathllm}
}
```

---

## Contact

- **Issues**: [GitHub Issues](https://github.com/yasinldev/mathllm/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yasinldev/mathllm/discussions)

---

**Status**: Active development | Last updated: October 2025
